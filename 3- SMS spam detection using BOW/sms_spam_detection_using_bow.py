# -*- coding: utf-8 -*-
"""2- SMS Spam Detection using BOW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NLA8mwVTvQu2diU6y-w6PQRmgTqpeAB9

# SMS spam detection

## Part one import basic libraries and read data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df=pd.read_csv(r"/content/drive/MyDrive/dataset/spam.csv",encoding='latin-1')

"""## Part two data description and EDA"""

df.head()

df.drop(['Unnamed: 2',	'Unnamed: 3',	'Unnamed: 4'],axis=1,inplace=True)
df.rename({'v1':'label','v2':'text'},axis=1,inplace=True)
df.head()

df.shape

df.info()

df.describe()

max_len = max(len(x) for x in df['text'])
max_len

df['length']=df['text'].apply(len)
df.head()

df['length'].aggregate([max,min,np.mean,np.median,np.std])

plt.hist(df['length'])
plt.xlabel('review length')
plt.ylabel('values')
plt.title('review length')
plt.grid()
plt.show()

df['label'].value_counts()

plt.bar(df['label'].value_counts().index, df['label'].value_counts())
plt.xlabel('labels')
plt.ylabel('values')
plt.title('Human VS Spam',weight='bold')
plt.grid()
plt.show()

"""## Part three data preprocessing and build vocab




"""

df['label'].replace({'ham':0,'spam':1},inplace=True)
df.head()

x=df['text']
x

y=df['label']
y

def remove_special_chars(text):
    re1 = re.compile(r'  +')
    x1 = text.lower().replace('#39;', "'").replace('amp;', '&').replace('#146;', "'").replace(
        'nbsp;', ' ').replace('#36;', '$').replace('\\n', "\n").replace('quot;', "'").replace(
        '<br />', "\n").replace('\\"', '"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(
        ' @-@ ', '-').replace('\\', ' \\ ')
    return re1.sub(' ', html.unescape(x1))



def remove_non_ascii(text):
    """Remove non-ASCII characters from list of tokenized words"""
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')


def remove_punctuation(text):
    """Remove punctuation from list of tokenized words"""
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)


def to_lowercase(text):
    return text.lower()


def replace_numbers(text):
    """Replace all interger occurrences in list of tokenized words with textual representation"""
    return re.sub(r'\d+', '', text)

def remove_whitespaces(text):
    return text.strip()


def remove_stopwords(words, stop_words):
    """
    :param words:
    :type words:
    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
    or
    from spacy.lang.en.stop_words import STOP_WORDS
    :type stop_words:
    :return:
    :rtype:
    """
    return [word for word in words if word not in stop_words]


def stem_words(words):
    """Stem words in text"""
    stemmer = PorterStemmer()
    return [stemmer.stem(word) for word in words]

def lemmatize_words(words):
    """Lemmatize words in text"""

    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in words]

def lemmatize_verbs(words):
    """Lemmatize verbs in text"""

    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])

def text2words(text):
  text=sent_tokenize(text)
  return word_tokenize("".join(text))

# clean scrapping
import html
import unicodedata
import string

# regex
import re

# tokenization
import nltk
nltk.download('punkt')

## sent_tokenize
from nltk.tokenize import sent_tokenize
## word_tokenize
from nltk.tokenize import word_tokenize

# remove stop words
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words=stopwords.words('english')

# stemming
from nltk.stem import PorterStemmer

# lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')


def normalize_text( text):
    text = remove_special_chars(text)
    text = remove_non_ascii(text)
    text = remove_punctuation(text)
    text = to_lowercase(text)
    text = replace_numbers(text)
    words = text2words(text)
    words = remove_stopwords(words, stop_words)
    #words = stem_words(words)# Either stem ovocar lemmatize

    #lemmatization is good in BOW
    words = lemmatize_words(words)
    words = lemmatize_verbs(words)

    return ''.join(words)

def normalize_corpus (corpus):
  return [normalize_text(s) for s in corpus]

texts=normalize_corpus(x)

"""#### build vocab using keras Tokenizer with mode freq"""

# fit the tokenizer

from keras.preprocessing.text import Tokenizer
vocab_sz=10000
tok=Tokenizer(num_words=vocab_sz,oov_token="UNK")
tok.fit_on_texts(texts)

tok.word_index

texts=tok.texts_to_matrix(texts,mode='freq')

y=np.asarray(y).astype('float32')

print (texts.shape)
print (y.shape)

"""## Part four split data to train validation test"""

x_train=texts[:int(len(texts)*.6)]
y_train=y[:int(len(y)*.6)]
x_val=texts[int(len(texts)*.6):int(len(texts)*.8)]
y_val=y[int(len(y)*.6):int(len(y)*.8)]
x_test=texts[int(len(texts)*.8):]
y_test=y[int(len(y)*.8):]


print (x_train.shape)
print (y_train.shape)
print (x_val.shape)
print (y_val.shape)
print (x_test.shape)
print (y_test.shape)

"""## Part five build network

#### 1- simple neural network model
"""

from keras.layers import Dense
from keras.models import Sequential

model=Sequential()
model.add(Dense(16,activation='relu',input_shape=(10000,)))
model.add(Dense(16,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.summary()

from keras.losses import binary_crossentropy
from keras.optimizers import RMSprop
from keras.metrics import binary_accuracy

model.compile(optimizer=RMSprop(lr=0.01),loss=binary_crossentropy,metrics=[binary_accuracy])

history=model.fit(x_train,y_train,validation_data=(x_val,y_val),batch_size=512,epochs=20)

"""#### Evaluate the model"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['loss','val_loss'])
plt.title("BOW loss VS val_loss",fontweight="bold")
plt.show()

from sklearn.metrics import accuracy_score

# Make predictions on test data
predictions = model.predict(x_train)

# Since the predictions are probabilities, we need to convert them to binary values
# You can use a threshold of 0.5 for binary classification
binary_predictions = (predictions > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_train, binary_predictions)

print("Accuracy on train data:", accuracy)

# Use the trained model to predict labels for the test data
predictions = model.predict(x_val)

# Convert predicted probabilities to binary labels (0 or 1)
predicted_labels = (predictions > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_val,predicted_labels)

print("Accuracy on test set:", accuracy)

# Make predictions on test data
predictions = model.predict(x_test)

# Since the predictions are probabilities, we need to convert them to binary values
# You can use a threshold of 0.5 for binary classification
binary_predictions = (predictions > 0.5).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_test, binary_predictions)

print("Accuracy on test data:", accuracy)

"""#### 2- naive bayes"""

x_train=texts[:int(len(texts)*.6)]
y_train=y[:int(len(y)*.6)]
x_test=texts[int(len(texts)*.6):]
y_test=y[int(len(y)*.6):]


print (x_train.shape)
print (y_train.shape)
print (x_test.shape)
print (y_test.shape)

from sklearn.naive_bayes import MultinomialNB
naive_bayes = MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
naive_bayes.fit(x_train,y_train)

predictions = naive_bayes.predict(x_test)

accuracy=accuracy_score(y_test,predictions)
accuracy